This uses Deep Q Learning implementation via pytorch for Lunar Landing Environment in Gymnasium.
The Lunar Lander is a classic reinforcement learning environment provided by OpenAIâ€™s Gym library. The goal is to develop an intelligent agent capable of landing a lunar module safely on the moon's surface while consuming minimal fuel. This article will explore solving this challenging problem using Deep Q-Networks (DQNs), a popular deep reinforcement learning technique.
The main objective of reinforcement learning (RL) is to enable an agent to act optimally to maximize the cumulative long-term reward. Q-learning is a model free RL algorithm, which iteratively learns a long-term reward function "Q" given the current state and action. The Deep Q-Learning Network (DQN) facilitates the Q-learning by modeling the Q-function as a neural network. This project implements and experiments such DQN models on the OpenAI Gym's LunarLander-v2 environment, using a two-layer feed-forward network with a technique named "experience replay". Extensive experiments are done to determine the neural network size and tune various hyper-parameters including learning rate \alpha, reward discount factor \gamma and exploration-exploitation trade-off \epsilon. Major findings include 1) the Lunar Lander favors a large hidden layer but not a deeper network; 2) a near-one reward discount is necessary for the model to consider final successful landing. Finally, our best model can stably achieve 200+ mean reward for a trial of 900 landing episodes.
